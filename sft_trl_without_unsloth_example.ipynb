{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth 的微调样例代码\n",
    "\n",
    "## 环境配置\n",
    "\n",
    "系统为`Ubuntu 20.04`\n",
    "CUDA为12.4，驱动是550.54.15\n",
    "\n",
    "### 安装Conda\n",
    "```bash\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "### 创建虚拟环境、添加一些镜像列表\n",
    "```bash\n",
    "# conda环境的镜像\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\n",
    "conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/free/\n",
    "conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/main/\n",
    "conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud/conda-forge/\n",
    "# pip镜像\n",
    "pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/\n",
    "pip config set global.index-url http://mirrors.cloud.tencent.com/pypi/simple\n",
    "pip config set global.index-url http://pypi.douban.com/simple/\n",
    "pip config set global.index-url https://mirrors.163.com/pypi/simple/\n",
    "# 创建虚拟环境\n",
    "conda create -n <环境名称> python=3.12\n",
    "# 激活你创建的环境\n",
    "conda activate <环境名称>\n",
    "```\n",
    "\n",
    "### 安装运行库\n",
    "```bash\n",
    "# pytorch安装\n",
    "pip install --upgrade torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124 --trusted-host download.pytorch.org\n",
    "# 其他的依赖安装\n",
    "pip install --upgrade torch==2.6.0 transformers BitsandBytes accelerate nltk numpy pandas tensorboardX evaluate scikit-learn sentence-transformers tiktoken deepspeed SentencePiece unsloth qwen-vl-utils[decord] nvitop trl -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# 如果用jupyter notebook 需要安装以下两个\n",
    "conda install -n testllm ipykernel --update-deps --force-reinstall\n",
    "conda install -n testllm IProgress\n",
    "```\n",
    "\n",
    "## 配置GPU、并引入包体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, argparse\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\" # 这里替换成你想要的GPU序号\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    LogitsProcessorList,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.trainer import TRAINER_STATE_NAME\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers.trainer_utils import SaveStrategy\n",
    "from transformers.tokenization_utils import PaddingStrategy\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n",
    "from peft.utils import (\n",
    "    CONFIG_NAME,\n",
    "    SAFETENSORS_WEIGHTS_NAME,\n",
    "    TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, IterableDataset\n",
    "from dataclasses import dataclass\n",
    "# from 模板 import get_template_and_model_path\n",
    "# from public_apis.file_rw import *\n",
    "from typing import Union, Optional, List, Dict, Any, Literal, Sequence, Tuple\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "IGNORE_INDEX=-100\n",
    "\n",
    "TEMPLATE_DICT={\n",
    "    'llama2':\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \\n{user_query}\\nAssistant:  \",\n",
    "    'llama3':\"<|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant. <|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\\n{user_query}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\",\n",
    "    'qwen2.5':\"<|im_start|>system\\nYou are a helpful assistant. <|im_end|>\\n<|im_start|>user\\n{user_query}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    'gpt2':\"{user_query}\\n\"\n",
    "}\n",
    "\n",
    "MODEL_BASE_PATH='/data'\n",
    "\n",
    "# MODEL_BASE_PATH='/data/lzy/models/LLM'\n",
    "\n",
    "MODEL_NAME_DICT={\n",
    "    'llama2-7b':os.path.join(MODEL_BASE_PATH,'llama','llama-2-7b-hf'),\n",
    "    'llama2-13b':os.path.join(MODEL_BASE_PATH,'llama','llama-2-13b-hf'),\n",
    "    'llama3.1-8b-i':os.path.join(MODEL_BASE_PATH,'llama','llama3.1-8b-instruct'),\n",
    "    'llama3.1-8b':os.path.join(MODEL_BASE_PATH,'llama','llama3.1-8b'),\n",
    "    'llama3.2-1b-i':os.path.join(MODEL_BASE_PATH,'llama','llama3.2-1B-Instruct'),\n",
    "    'llama3.2-3b-i':os.path.join(MODEL_BASE_PATH,'llama','llama3.2-3B-Instruct'),\n",
    "    'qwen2.5-0.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-0.5B-Instruct'),\n",
    "    'qwen2.5-1.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-1.5B-Instruct'),\n",
    "    'qwen2.5-3b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-3B-Instruct'),\n",
    "    'qwen2.5-7b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-7B-Instruct'),\n",
    "    'qwen2.5-14b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-14B-Instruct'),\n",
    "    'qwen2.5-coder-0.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-0.5B-Instruct'),\n",
    "    'qwen2.5-coder-1.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-1.5B-Instruct'),\n",
    "    'qwen2.5-coder-3b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-3B-Instruct'),\n",
    "    'qwen2.5-coder-7b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-7B-Instruct'),\n",
    "    'qwen2.5-coder-14b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-14B-Instruct'),\n",
    "    # 'gpt2l':os.path.join(MODEL_BASE_PATH,'openai-community','gpt2-large'),\n",
    "    # 'gpt2':os.path.join(MODEL_BASE_PATH,'openai-community','gpt2')\n",
    "}\n",
    "\n",
    "def get_template_and_model_path(**arguments):\n",
    "    model_name=str(arguments.get('model_name','llama3.1-8b-i'))\n",
    "    model_name_or_path=MODEL_NAME_DICT[model_name]\n",
    "    if model_name.startswith('qwen2.5'):\n",
    "        template=TEMPLATE_DICT['qwen2.5']\n",
    "    elif model_name.startswith('llama2'):\n",
    "        template=TEMPLATE_DICT['llama2']\n",
    "    elif model_name.startswith('gpt2'):\n",
    "        template=TEMPLATE_DICT['gpt2']\n",
    "    else:\n",
    "        template=TEMPLATE_DICT['llama3']\n",
    "    return model_name,model_name_or_path,template\n",
    "\n",
    "\n",
    "# 自定义一种错误形式，主要用于防呆以及简化操作\n",
    "class CustomDatasetTokenizerError(Exception):\n",
    "    \"\"\"Base class for custom exceptions in this module.\"\"\"\n",
    "    def __init__(self):\n",
    "        # self.expression = expression\n",
    "        self.message = \"数据集元素的输入键值必须是以下组合：\\n\\t\\\"prompt\\\",\\\"query\\\",\\\"response\\\"\\n\\t\\\"instruction\\\",\\\"input\\\",\\\"output\\\",\\n\\t\\\"prompt\\\",\\\"completion\\\"\"\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "    pass\n",
    "# prompt_template应该是中间有个关键词叫做 `user_query` \n",
    "# 这个函数用于常规的微调（手搓，然后用`transformers`中的`trainer`或者`seq2seqtrainer`直接微调）\n",
    "# 从载入的JSON文件一步变为令牌化的数据集样式\n",
    "def process_tokens_tokenizer_functions(example: dict[str, str],tokenizer: Union[PreTrainedTokenizer,PreTrainedTokenizerBase,PreTrainedTokenizerFast,AutoTokenizer],prompt_template: str,train_mode: Literal[\"sft\", \"pt\"] = \"sft\"):\n",
    "    prompt=\"\"\n",
    "    completion=\"\"\n",
    "    if \"prompt\" in example.keys() and \"query\" in example.keys() and \"response\" in example.keys():\n",
    "        prompt=prompt_template.format(user_query=\"{}\\n{}\\n\".format(example[\"prompt\"], example[\"query\"]))\n",
    "        completion=example[\"response\"]\n",
    "    elif \"instruction\" in example.keys() and \"input\" in example.keys() and \"output\" in example.keys():\n",
    "        prompt=prompt_template.format(user_query=\"{}\\n{}\\n\".format(example[\"instruction\"], example[\"input\"]))\n",
    "        completion=example[\"output\"]\n",
    "    elif \"prompt\" in example.keys() and \"completion\" in example.keys():\n",
    "        prompt=prompt_template.format(user_query=\"{}\\n\".format(example[\"prompt\"]))\n",
    "        completion=example[\"completion\"]\n",
    "    else:\n",
    "        raise CustomDatasetTokenizerError()\n",
    "    # SFT样本构建\n",
    "    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    # 找出非标签部分\n",
    "    example_str = \"{}{}\".format(prompt, completion)\n",
    "    input_id = tokenizer.encode(example_str)\n",
    "    # 训练数据集末尾带上截止符号\n",
    "    if input_id[-1] != tokenizer.eos_token_id:\n",
    "        input_id.append(tokenizer.eos_token_id)\n",
    "    input_id_before_label = tokenizer.encode(prompt)\n",
    "    label = input_id.copy()\n",
    "    # SFT只需要label部分计算梯度，pt需要所有部分计算梯度\n",
    "    if train_mode == \"sft\":\n",
    "        label[: len(input_id_before_label)] = [IGNORE_INDEX] * len(\n",
    "            input_id_before_label\n",
    "        )\n",
    "    attention_mask = [1] * len(input_id)\n",
    "    # 放入输入字典中\n",
    "    if \"qid\" in example.keys():\n",
    "        model_inputs[\"qid\"] = example[\"qid\"]\n",
    "    elif \"ID\" in example.keys():\n",
    "        model_inputs[\"qid\"] = example[\"ID\"]\n",
    "    model_inputs[\"input_ids\"] = input_id\n",
    "    model_inputs[\"attention_mask\"] = attention_mask\n",
    "    model_inputs[\"labels\"] = label\n",
    "    return model_inputs\n",
    "# 直接处理成`trl`可接受的格式（直接文本就行，剩下的交给剩下）\n",
    "def process_dataset_functions(example: dict[str, str]):\n",
    "    prompt=\"\"\n",
    "    completion=\"\"\n",
    "    if \"prompt\" in example.keys() and \"query\" in example.keys() and \"response\" in example.keys():\n",
    "        prompt=\"{}\\n{}\\n\".format(example[\"prompt\"], example[\"query\"])\n",
    "        completion=example[\"response\"]\n",
    "    elif \"instruction\" in example.keys() and \"input\" in example.keys() and \"output\" in example.keys():\n",
    "        prompt=\"{}\\n{}\\n\".format(example[\"instruction\"], example[\"input\"])\n",
    "        completion=example[\"output\"]\n",
    "    elif \"prompt\" in example.keys() and \"completion\" in example.keys():\n",
    "        prompt=\"{}\\n\".format(example[\"prompt\"])\n",
    "        completion=example[\"completion\"]\n",
    "    else:\n",
    "        raise CustomDatasetTokenizerError()\n",
    "    return {\"prompt\":prompt,\"completion\":completion,\"text\":\"### Instruction:\\n {}\\n ### Response:\\n {}\\n\".format(prompt,completion),\"ground_truth\":completion}\n",
    "# 查找检查点需要的正则表达式\n",
    "CHECKPOINT_FOLD = re.compile(r\"(?:checkpoint\\-\\d+)\")\n",
    "FINALMODEL_NAME = re.compile(r\"(?:(?:(?:adapter|pytorch)_)?model(?:\\-\\d+)?\\.(?:safetensors|bin))\")\n",
    "# 查找检查点的函数\n",
    "def checkout_format(string: str, pattern: re.Pattern):\n",
    "    result = [n for n in pattern.findall(string) if n]\n",
    "    if len(result) == 1:\n",
    "        return result[0] == string\n",
    "    return False\n",
    "# 查找检查点的函数\n",
    "def checkpoint_sort_func(checkpoint_dirname: str):\n",
    "    items = [n.strip() for n in checkpoint_dirname.split(\"-\") if n.strip()]\n",
    "    return int(items[-1])\n",
    "# 查找检查点的函数\n",
    "def findout_checkpoint(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    items = os.listdir(path)\n",
    "    saved_files = [n for n in items if checkout_format(n, FINALMODEL_NAME)]\n",
    "    if len(saved_files) > 0:\n",
    "        return path\n",
    "    check_dirs = [n for n in items if checkout_format(n, CHECKPOINT_FOLD)]\n",
    "    if len(check_dirs) < 1:\n",
    "        return None\n",
    "    check_dirs.sort(key=lambda x: checkpoint_sort_func(x), reverse=True)\n",
    "    for f in check_dirs:\n",
    "        result = findout_checkpoint(os.path.join(path, f))\n",
    "        if result != None:\n",
    "            return result\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/miniconda3/envs/kbqa/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5974e476fe1d4dddb7b7747dfa6e6694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name, model_name_or_path, template = get_template_and_model_path(\n",
    "    model_name=\"llama3.1-8b-i\"\n",
    ")\n",
    "\n",
    "output_dir = \"results/sft_trl_none_unsloth\"\n",
    "\n",
    "config_dicts = dict(\n",
    "    ignore_pad_token_for_loss=True,\n",
    "    lora_config_dicts=dict(\n",
    "        lora_rank=8, lora_alpha=32, lora_dropout=0.1, additional_target=None\n",
    "    ),\n",
    ")\n",
    "\n",
    "training_config_dict = dict(\n",
    "    data_path=\"/data1/SG_KBQA/SG_KBQA/gen_dataset/webqsp/train_examples.json\",\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=False,\n",
    "    do_train=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=20,\n",
    "    save_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "    logging_steps=1000,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    resume_from_checkpoint=False,\n",
    "    warmup_ratio=0,\n",
    "    gradient_checkpointing=False,\n",
    ")\n",
    "eval_config_dict = dict(\n",
    "    data_path=\"/data1/SG_KBQA/SG_KBQA/gen_dataset/webqsp/dev_examples.json\",\n",
    "    max_new_tokens=2048,\n",
    ")\n",
    "\n",
    "\n",
    "generating_args = {\n",
    "    \"val_size\": eval_config_dict.get(\"val_size\", 0),\n",
    "    \"streaming\": eval_config_dict.get(\"streaming\", False),\n",
    "    \"buffer_size\": eval_config_dict.get(\"buffer_size\", 16384),\n",
    "    \"do_sample\": eval_config_dict.get(\"do_sample\", True),\n",
    "    \"temperature\": eval_config_dict.get(\"temperature\", 1),\n",
    "    \"top_p\": eval_config_dict.get(\"top_p\", 0.7),\n",
    "    \"top_k\": eval_config_dict.get(\"top_k\", 50),\n",
    "    \"num_beams\": eval_config_dict.get(\n",
    "        \"num_beams\", 8\n",
    "    ),  # 训练的时候为`None`，推理的时候大于0\n",
    "    \"max_new_tokens\": eval_config_dict.get(\"max_new_tokens\", 1024),\n",
    "    \"repetition_penalty\": eval_config_dict.get(\"repetition_penalty\", 1.0),\n",
    "    \"length_penalty\": eval_config_dict.get(\"length_penalty\", 1.0),\n",
    "    # \"num_beam_groups\":4,\n",
    "    # \"diversity_penalty\":0.2\n",
    "}\n",
    "\n",
    "lora_config_dicts = config_dicts[\"lora_config_dicts\"]\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_config_dicts[\"lora_rank\"],\n",
    "    lora_alpha=lora_config_dicts[\"lora_alpha\"],\n",
    "    lora_dropout=lora_config_dicts[\"lora_dropout\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    modules_to_save=lora_config_dicts.get(\"additional_target\", None),\n",
    ")\n",
    "prompt_template = template\n",
    "\n",
    "model_path = model_name_or_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"cuda\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, lora_config, \"default\")\n",
    "loadpath = findout_checkpoint(output_dir)\n",
    "if loadpath != None:\n",
    "    print(\"从`{}`加载lora头\".format(loadpath))\n",
    "    model.load_adapter(loadpath, \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qid': 'WebQTrn-3581', 'instruction': 'Please translate the following question into logical form using the provided relations and entities.', 'input': 'Question: where is university of auburn\\nCandidate relations with their corresponding domain class [D], name [N], range class [R]:\\n[D] education.education [N] education.education.institution [R] education.educational_institution\\n[D] people.person [N] people.person.education [R] education.education\\n[D] location.mailing_address [N] location.mailing_address.citytown [R] location.location\\n[D] common.topic [N] common.topic.notable_types [R] type.type\\n[D] people.person [N] people.person.place_of_birth [R] location.location\\n[D] organization.organization [N] organization.organization.headquarters [R] location.mailing_address\\n[D] location.location [N] location.location.containedby [R] location.location\\n[D] location.mailing_address [N] location.mailing_address.country [R] location.country\\n[D] location.mailing_address [N] location.mailing_address.state_province_region [R] location.administrative_division\\n[D] location.location [N] location.location.contains [R] location.location\\n[D] sports.sports_team [N] sports.sports_team.location [R] sports.sports_team_location\\n[D] location.location [N] location.location.adjoin_s [R] location.adjoining_relationship\\n[D] location.country [N] location.country.form_of_government [R] government.form_of_government\\n[D] government.governmental_jurisdiction [N] government.governmental_jurisdiction.governing_officials [R] government.government_position_held\\n[D] location.hud_county_place [N] location.hud_county_place.county [R] location.us_county\\n[D] education.educational_institution [N] education.educational_institution.school_type [R] education.school_category\\n[D] location.mailing_address [N] location.mailing_address.street_address [R] type.text\\n[D] education.education [N] education.education.degree [R] education.educational_degree\\n[D] base.biblioness.bibs_location [N] base.biblioness.bibs_location.state [R] location.location\\n[D] location.location [N] location.location.primarily_containedby [R] location.location\\nCandidate entities with their corresponding id [ID], name [N], class [C]:\\n[ID] m.01wdj_ [N] Auburn University [C] education.educational_institution ; location.location ; common.topic ; organization.organization\\n[ID] m.05ytbh [N] Auburn University at Montgomery [C] education.educational_institution ; location.location ; common.topic ; organization.organization\\n[ID] m.0d2jw2 [N] Rehabilitation counseling [C] common.topic\\n', 'output': '(JOIN (R location.location.containedby) m.01wdj_)'}\n",
      "{'prompt': 'Please translate the following question into logical form using the provided relations and entities.\\nQuestion: where is university of auburn\\nCandidate relations with their corresponding domain class [D], name [N], range class [R]:\\n[D] education.education [N] education.education.institution [R] education.educational_institution\\n[D] people.person [N] people.person.education [R] education.education\\n[D] location.mailing_address [N] location.mailing_address.citytown [R] location.location\\n[D] common.topic [N] common.topic.notable_types [R] type.type\\n[D] people.person [N] people.person.place_of_birth [R] location.location\\n[D] organization.organization [N] organization.organization.headquarters [R] location.mailing_address\\n[D] location.location [N] location.location.containedby [R] location.location\\n[D] location.mailing_address [N] location.mailing_address.country [R] location.country\\n[D] location.mailing_address [N] location.mailing_address.state_province_region [R] location.administrative_division\\n[D] location.location [N] location.location.contains [R] location.location\\n[D] sports.sports_team [N] sports.sports_team.location [R] sports.sports_team_location\\n[D] location.location [N] location.location.adjoin_s [R] location.adjoining_relationship\\n[D] location.country [N] location.country.form_of_government [R] government.form_of_government\\n[D] government.governmental_jurisdiction [N] government.governmental_jurisdiction.governing_officials [R] government.government_position_held\\n[D] location.hud_county_place [N] location.hud_county_place.county [R] location.us_county\\n[D] education.educational_institution [N] education.educational_institution.school_type [R] education.school_category\\n[D] location.mailing_address [N] location.mailing_address.street_address [R] type.text\\n[D] education.education [N] education.education.degree [R] education.educational_degree\\n[D] base.biblioness.bibs_location [N] base.biblioness.bibs_location.state [R] location.location\\n[D] location.location [N] location.location.primarily_containedby [R] location.location\\nCandidate entities with their corresponding id [ID], name [N], class [C]:\\n[ID] m.01wdj_ [N] Auburn University [C] education.educational_institution ; location.location ; common.topic ; organization.organization\\n[ID] m.05ytbh [N] Auburn University at Montgomery [C] education.educational_institution ; location.location ; common.topic ; organization.organization\\n[ID] m.0d2jw2 [N] Rehabilitation counseling [C] common.topic\\n\\n', 'completion': '(JOIN (R location.location.containedby) m.01wdj_)', 'text': '### Instruction:\\n Please translate the following question into logical form using the provided relations and entities.\\nQuestion: where is university of auburn\\nCandidate relations with their corresponding domain class [D], name [N], range class [R]:\\n[D] education.education [N] education.education.institution [R] education.educational_institution\\n[D] people.person [N] people.person.education [R] education.education\\n[D] location.mailing_address [N] location.mailing_address.citytown [R] location.location\\n[D] common.topic [N] common.topic.notable_types [R] type.type\\n[D] people.person [N] people.person.place_of_birth [R] location.location\\n[D] organization.organization [N] organization.organization.headquarters [R] location.mailing_address\\n[D] location.location [N] location.location.containedby [R] location.location\\n[D] location.mailing_address [N] location.mailing_address.country [R] location.country\\n[D] location.mailing_address [N] location.mailing_address.state_province_region [R] location.administrative_division\\n[D] location.location [N] location.location.contains [R] location.location\\n[D] sports.sports_team [N] sports.sports_team.location [R] sports.sports_team_location\\n[D] location.location [N] location.location.adjoin_s [R] location.adjoining_relationship\\n[D] location.country [N] location.country.form_of_government [R] government.form_of_government\\n[D] government.governmental_jurisdiction [N] government.governmental_jurisdiction.governing_officials [R] government.government_position_held\\n[D] location.hud_county_place [N] location.hud_county_place.county [R] location.us_county\\n[D] education.educational_institution [N] education.educational_institution.school_type [R] education.school_category\\n[D] location.mailing_address [N] location.mailing_address.street_address [R] type.text\\n[D] education.education [N] education.education.degree [R] education.educational_degree\\n[D] base.biblioness.bibs_location [N] base.biblioness.bibs_location.state [R] location.location\\n[D] location.location [N] location.location.primarily_containedby [R] location.location\\nCandidate entities with their corresponding id [ID], name [N], class [C]:\\n[ID] m.01wdj_ [N] Auburn University [C] education.educational_institution ; location.location ; common.topic ; organization.organization\\n[ID] m.05ytbh [N] Auburn University at Montgomery [C] education.educational_institution ; location.location ; common.topic ; organization.organization\\n[ID] m.0d2jw2 [N] Rehabilitation counseling [C] common.topic\\n\\n\\n ### Response:\\n (JOIN (R location.location.containedby) m.01wdj_)\\n', 'ground_truth': '(JOIN (R location.location.containedby) m.01wdj_)'}\n",
      "<|eot_id|>\n",
      "<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/miniconda3/envs/kbqa/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You are passing `packing=True` and `padding_free=True` which is not recommended. Please refer to the documentation to understand why this is not recommended.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-09 10:54:26,506] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzy/miniconda3/envs/kbqa/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/lzy/miniconda3/envs/kbqa/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-09 10:54:27,531] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='1120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 160/1120 35:27 < 3:35:28, 0.07 it/s, Epoch 2.85/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.964300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.895400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.877200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.909400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.845100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.821600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.787300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.786900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.737800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.666100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.630300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.616900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.585900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.608700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.570400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.546500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.552300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.511500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.488100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.459800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.471700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.454400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.451100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.471500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.424700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.435200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.432300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.446900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.411600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.404900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.426700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.415700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.418500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.385700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.388400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.400600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.376900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.406700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.373300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.395300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.384100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.386600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.363500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.385700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.373600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.393400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"/data1/SG_KBQA/SG_KBQA/gen_dataset/webqsp\", split=\"train\")\n",
    "print(dataset[0])\n",
    "# dataset_processed=dataset.map(lambda example: {\"prompt\": \"{}\\n{}\\n\".format(example['instruction'],example['input']), \"completion\":example['output'] ,\"text\":\"### Prompt:\\n {}\\n{}\\n ### Completion:\\n {}\\n\".format(example['instruction'],example['input'],example['output'])})\n",
    "dataset_processed=dataset.map(lambda example: process_dataset_functions(example))\n",
    "dataset_processed=dataset_processed.remove_columns(dataset.column_names)\n",
    "print(dataset_processed[0])\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    # tokenizer = tokenizer,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset = dataset_processed,\n",
    "    # dataset_text_field = \"text\",\n",
    "    # max_seq_length = 2048,\n",
    "    # dataset_num_proc = 2,    \n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 20, # Set this for 1 full training run.\n",
    "        save_steps=1000,\n",
    "        save_strategy=SaveStrategy.STEPS,\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 1e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        activation_offloading=True,\n",
    "        completion_only_loss=True,\n",
    "        max_seq_length=2048,\n",
    "        dataset_num_proc = 24,\n",
    "        packing = True, # Can make training 5x faster for short sequences.\n",
    "        padding_free=True,\n",
    "        # label_names =\n",
    "    ),\n",
    "    # formatting_func= lambda example: \"### Prompt:\\n {}\\n ### Completion:\\n {}\\n\".format(example['prompt'],example['completion'])\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
