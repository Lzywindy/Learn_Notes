{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth 的微调样例代码\n",
    "\n",
    "## 环境配置\n",
    "\n",
    "系统为`Ubuntu 20.04`\n",
    "CUDA为12.4，驱动是550.54.15\n",
    "\n",
    "### 安装Conda\n",
    "```bash\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "### 创建虚拟环境、添加一些镜像列表\n",
    "```bash\n",
    "# conda环境的镜像\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\n",
    "conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/free/\n",
    "conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/main/\n",
    "conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud/conda-forge/\n",
    "# pip镜像\n",
    "pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/\n",
    "pip config set global.index-url http://mirrors.cloud.tencent.com/pypi/simple\n",
    "pip config set global.index-url http://pypi.douban.com/simple/\n",
    "pip config set global.index-url https://mirrors.163.com/pypi/simple/\n",
    "# 创建虚拟环境\n",
    "conda create -n <环境名称> python=3.12\n",
    "# 激活你创建的环境\n",
    "conda activate <环境名称>\n",
    "```\n",
    "\n",
    "### 安装运行库\n",
    "```bash\n",
    "# pytorch安装\n",
    "pip install --upgrade torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124 --trusted-host download.pytorch.org\n",
    "# 其他的依赖安装\n",
    "pip install --upgrade torch==2.6.0 transformers BitsandBytes accelerate nltk numpy pandas tensorboardX evaluate scikit-learn sentence-transformers tiktoken deepspeed SentencePiece unsloth qwen-vl-utils[decord] nvitop trl -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# 如果用jupyter notebook 需要安装以下两个\n",
    "conda install -n testllm ipykernel --update-deps --force-reinstall\n",
    "conda install -n testllm IProgress\n",
    "```\n",
    "\n",
    "## 配置GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, argparse\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # 这里替换成你想要的GPU序号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入`unsloth`包，需要在`transformers`、`peft`和`trl`之前引入以方便`unsloth`进行加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    LogitsProcessorList,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.trainer import TRAINER_STATE_NAME\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers.trainer_utils import SaveStrategy\n",
    "from transformers.tokenization_utils import PaddingStrategy\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n",
    "from peft.utils import (\n",
    "    CONFIG_NAME,\n",
    "    SAFETENSORS_WEIGHTS_NAME,\n",
    "    TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n",
    ")\n",
    "IGNORE_INDEX = -100 # 引入这个常数，有时候在梯度下降时用于屏蔽无需计算梯度的token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入数据集和数据处理函数还有自定义的模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, IterableDataset\n",
    "from dataclasses import dataclass\n",
    "# from 模板 import get_template_and_model_path\n",
    "# from public_apis.file_rw import *\n",
    "from typing import Union, Optional, List, Dict, Any, Literal, Sequence, Tuple\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "IGNORE_INDEX=-100\n",
    "\n",
    "TEMPLATE_DICT={\n",
    "    'llama2':\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \\n{user_query}\\nAssistant:  \",\n",
    "    'llama3':\"<|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant. <|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\\n{user_query}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\",\n",
    "    'qwen2.5':\"<|im_start|>system\\nYou are a helpful assistant. <|im_end|>\\n<|im_start|>user\\n{user_query}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    'gpt2':\"{user_query}\\n\"\n",
    "}\n",
    "\n",
    "MODEL_BASE_PATH='/data'\n",
    "\n",
    "# MODEL_BASE_PATH='/data/lzy/models/LLM'\n",
    "\n",
    "MODEL_NAME_DICT={\n",
    "    'llama2-7b':os.path.join(MODEL_BASE_PATH,'llama','llama-2-7b-hf'),\n",
    "    'llama2-13b':os.path.join(MODEL_BASE_PATH,'llama','llama-2-13b-hf'),\n",
    "    'llama3.1-8b-i':os.path.join(MODEL_BASE_PATH,'llama','llama3.1-8b-instruct'),\n",
    "    'llama3.1-8b':os.path.join(MODEL_BASE_PATH,'llama','llama3.1-8b'),\n",
    "    'llama3.2-1b-i':os.path.join(MODEL_BASE_PATH,'llama','llama3.2-1B-Instruct'),\n",
    "    'llama3.2-3b-i':os.path.join(MODEL_BASE_PATH,'llama','llama3.2-3B-Instruct'),\n",
    "    'qwen2.5-0.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-0.5B-Instruct'),\n",
    "    'qwen2.5-1.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-1.5B-Instruct'),\n",
    "    'qwen2.5-3b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-3B-Instruct'),\n",
    "    'qwen2.5-7b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-7B-Instruct'),\n",
    "    'qwen2.5-14b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-14B-Instruct'),\n",
    "    'qwen2.5-coder-0.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-0.5B-Instruct'),\n",
    "    'qwen2.5-coder-1.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-1.5B-Instruct'),\n",
    "    'qwen2.5-coder-3b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-3B-Instruct'),\n",
    "    'qwen2.5-coder-7b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-7B-Instruct'),\n",
    "    'qwen2.5-coder-14b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-14B-Instruct'),\n",
    "    # 'gpt2l':os.path.join(MODEL_BASE_PATH,'openai-community','gpt2-large'),\n",
    "    # 'gpt2':os.path.join(MODEL_BASE_PATH,'openai-community','gpt2')\n",
    "}\n",
    "\n",
    "def get_template_and_model_path(**arguments):\n",
    "    model_name=str(arguments.get('model_name','llama3.1-8b-i'))\n",
    "    model_name_or_path=MODEL_NAME_DICT[model_name]\n",
    "    if model_name.startswith('qwen2.5'):\n",
    "        template=TEMPLATE_DICT['qwen2.5']\n",
    "    elif model_name.startswith('llama2'):\n",
    "        template=TEMPLATE_DICT['llama2']\n",
    "    elif model_name.startswith('gpt2'):\n",
    "        template=TEMPLATE_DICT['gpt2']\n",
    "    else:\n",
    "        template=TEMPLATE_DICT['llama3']\n",
    "    return model_name,model_name_or_path,template\n",
    "\n",
    "\n",
    "# 自定义一种错误形式，主要用于防呆以及简化操作\n",
    "class CustomDatasetTokenizerError(Exception):\n",
    "    \"\"\"Base class for custom exceptions in this module.\"\"\"\n",
    "    def __init__(self):\n",
    "        # self.expression = expression\n",
    "        self.message = \"数据集元素的输入键值必须是以下组合：\\n\\t\\\"prompt\\\",\\\"query\\\",\\\"response\\\"\\n\\t\\\"instruction\\\",\\\"input\\\",\\\"output\\\",\\n\\t\\\"prompt\\\",\\\"completion\\\"\"\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "    pass\n",
    "# prompt_template应该是中间有个关键词叫做 `user_query` \n",
    "# 这个函数用于常规的微调（手搓，然后用`transformers`中的`trainer`或者`seq2seqtrainer`直接微调）\n",
    "# 从载入的JSON文件一步变为令牌化的数据集样式\n",
    "def process_tokens_tokenizer_functions(example: dict[str, str],tokenizer: Union[PreTrainedTokenizer,PreTrainedTokenizerBase,PreTrainedTokenizerFast,AutoTokenizer],prompt_template: str,train_mode: Literal[\"sft\", \"pt\"] = \"sft\"):\n",
    "    prompt=\"\"\n",
    "    completion=\"\"\n",
    "    if \"prompt\" in example.keys() and \"query\" in example.keys() and \"response\" in example.keys():\n",
    "        prompt=prompt_template.format(user_query=\"{}\\n{}\\n\".format(example[\"prompt\"], example[\"query\"]))\n",
    "        completion=example[\"response\"]\n",
    "    elif \"instruction\" in example.keys() and \"input\" in example.keys() and \"output\" in example.keys():\n",
    "        prompt=prompt_template.format(user_query=\"{}\\n{}\\n\".format(example[\"instruction\"], example[\"input\"]))\n",
    "        completion=example[\"output\"]\n",
    "    elif \"prompt\" in example.keys() and \"completion\" in example.keys():\n",
    "        prompt=prompt_template.format(user_query=\"{}\\n\".format(example[\"prompt\"]))\n",
    "        completion=example[\"completion\"]\n",
    "    else:\n",
    "        raise CustomDatasetTokenizerError()\n",
    "    # SFT样本构建\n",
    "    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    # 找出非标签部分\n",
    "    example_str = \"{}{}\".format(prompt, completion)\n",
    "    input_id = tokenizer.encode(example_str)\n",
    "    # 训练数据集末尾带上截止符号\n",
    "    if input_id[-1] != tokenizer.eos_token_id:\n",
    "        input_id.append(tokenizer.eos_token_id)\n",
    "    input_id_before_label = tokenizer.encode(prompt)\n",
    "    label = input_id.copy()\n",
    "    # SFT只需要label部分计算梯度，pt需要所有部分计算梯度\n",
    "    if train_mode == \"sft\":\n",
    "        label[: len(input_id_before_label)] = [IGNORE_INDEX] * len(\n",
    "            input_id_before_label\n",
    "        )\n",
    "    attention_mask = [1] * len(input_id)\n",
    "    # 放入输入字典中\n",
    "    if \"qid\" in example.keys():\n",
    "        model_inputs[\"qid\"] = example[\"qid\"]\n",
    "    elif \"ID\" in example.keys():\n",
    "        model_inputs[\"qid\"] = example[\"ID\"]\n",
    "    model_inputs[\"input_ids\"] = input_id\n",
    "    model_inputs[\"attention_mask\"] = attention_mask\n",
    "    model_inputs[\"labels\"] = label\n",
    "    return model_inputs\n",
    "# 直接处理成`trl`可接受的格式（直接文本就行，剩下的交给剩下）\n",
    "def process_dataset_functions(example: dict[str, str]):\n",
    "    prompt=\"\"\n",
    "    completion=\"\"\n",
    "    if \"prompt\" in example.keys() and \"query\" in example.keys() and \"response\" in example.keys():\n",
    "        prompt=\"{}\\n{}\\n\".format(example[\"prompt\"], example[\"query\"])\n",
    "        completion=example[\"response\"]\n",
    "    elif \"instruction\" in example.keys() and \"input\" in example.keys() and \"output\" in example.keys():\n",
    "        prompt=\"{}\\n{}\\n\".format(example[\"instruction\"], example[\"input\"])\n",
    "        completion=example[\"output\"]\n",
    "    elif \"prompt\" in example.keys() and \"completion\" in example.keys():\n",
    "        prompt=\"{}\\n\".format(example[\"prompt\"])\n",
    "        completion=example[\"completion\"]\n",
    "    else:\n",
    "        raise CustomDatasetTokenizerError()\n",
    "    return {\"prompt\":prompt,\"completion\":completion,\"text\":\"### Instruction:\\n {}\\n ### Response:\\n {}\\n\".format(prompt,completion),\"ground_truth\":completion}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, model_name_or_path, template = get_template_and_model_path(\n",
    "    model_name=\"llama3.1-8b-i\"\n",
    ")\n",
    "\n",
    "# Load model\n",
    "max_length=2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name_or_path,\n",
    "    max_seq_length=max_length,\n",
    "    dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    # load_in_4bit=True,  # Use 4bit quantization to reduce memory usage. Can be False\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\"\n",
    "    ],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,  # Dropout = 0 is currently optimized\n",
    "    bias=\"none\",  # Bias = \"none\" is currently optimized\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "输入的数据集得的结构是以下三种类型的：\n",
    "\n",
    "1. 键值同时包含（第一优先级）： `prompt`,`query`,`response`\n",
    "2. 键值同时包含（第二优先级）： `instruction`,`input`,`output`\n",
    "3. 键值同时包含（第三优先级）： `prompt`,`completion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"<你的数据集位置>\", split=\"train\")\n",
    "print(dataset[0])\n",
    "# dataset_processed=dataset.map(lambda example: {\"prompt\": \"{}\\n{}\\n\".format(example['instruction'],example['input']), \"completion\":example['output'] ,\"text\":\"### Prompt:\\n {}\\n{}\\n ### Completion:\\n {}\\n\".format(example['instruction'],example['input'],example['output'])})\n",
    "dataset_processed=dataset.map(lambda example: process_dataset_functions(example))\n",
    "dataset_processed=dataset_processed.remove_columns(dataset.column_names)\n",
    "print(dataset_processed[0])\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_processed,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = True, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 20, # Set this for 1 full training run.\n",
    "        save_steps=1000,\n",
    "        save_strategy=SaveStrategy.STEPS,\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        completion_only_loss=True,\n",
    "    ),\n",
    "    # formatting_func= lambda example: \"### Prompt:\\n {}\\n ### Completion:\\n {}\\n\".format(example['prompt'],example['completion'])\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
