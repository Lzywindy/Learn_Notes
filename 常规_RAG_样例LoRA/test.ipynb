{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG简单工作流\n",
    "\n",
    "## 环境\n",
    "\n",
    "python 3.12  (>3.10就好)\n",
    "torch == 2.6.0 (建议)\n",
    "transformers >= 4.52.0 (建议)\n",
    "hnswlib \n",
    "\n",
    "## 文本数据格式\n",
    "\n",
    "json列表格式, 如下：\n",
    "\n",
    "[\n",
    "    str1,\n",
    "    str2,\n",
    "    str3,\n",
    "    ...\n",
    "]\n",
    "\n",
    "## 问答输入\n",
    "\n",
    "格式也是可以是`list[str]`，也可以是`str`，当是`list[str]`时，默认推理模型的`batch_size`为4\n",
    "\n",
    "## 推理输出\n",
    "\n",
    "`list[dict[str,Any]]`\n",
    "\n",
    "[\n",
    "    {\n",
    "        'query': <输入进来的问题>,\n",
    "        'response': <最终模型输出的答案>,\n",
    "        'search_result':<模型检索向量库时产生的结果>\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\n",
    "\n",
    "## 指令模板样式\n",
    "\n",
    "\"请依据所给的问题以及文本材料，生成最合适的答案。\\n\\n问题:\\n{query}\\n\\n文本材料:\\n{evidences}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, argparse\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # 这里替换成你想要的GPU序号\n",
    "\n",
    "from Components import *\n",
    "\n",
    "# 配置模型参数\n",
    "config=dict(\n",
    "    embedding_model_or_path=\"J:\\\\我的博士论文\\\\LLMs\\\\BAAI\\\\bge-m3\",  # 你自己的文本嵌入模型地址\n",
    "    embedding_text_source=\"data\\\\Text.json\",       # 用于检索的文本数据源\n",
    "    embedding_gpu=\"cuda:0\",   # 其实如果初次嵌入好了之后就没必要再用GPU了，相似度计算什么的都可以跑在CPU上\n",
    "    embedding_batch_size=16,   # 模型进行嵌入操作的时候的批次大小\n",
    "    inference_model_or_path=\"J:\\\\我的博士论文\\\\LLMs\\\\Qwen\\\\Qwen2.5-Coder-7B-Instruct\",  # 你自己的推理模型地址\n",
    "    inference_gpu=\"cuda:1\",   # 如果使用巨TM大的LLM推理，这里直接填`cuda`就好 （如果是其他环境就按照其他环境的参数写，比如华为的填写`npu`）\n",
    "    QInt4=False,             # 是否使用QInt4量化（使用的话用于减少显存占用）\n",
    "    lora_config=dict(\n",
    "        path=\"<your_lora_path>\",\n",
    "        lora_head=\"<your_lora_head_name>\",\n",
    "        lora_rank=8,\n",
    "        lora_dropout=32,\n",
    "        lora_dropout=0.1\n",
    "    ),\n",
    ")\n",
    "# 创建RAG工作流\n",
    "rag_worlflow=SimpleRAG_Workflow(**config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs=dict(max_new_tokens=2048,batch_size=4)\n",
    "prompt_prefix=\"请依据所给的问题以及文本材料，生成最合适的答案。\\n\\n问题:\\n{query}\\n\\n文本材料:\\n{evidences}\\n\\n\"\n",
    "qas=load_from_json(\"data\\\\qas.json\")\n",
    "results=rag_worlflow.query(prompt_prefix,[n['问题'] for n in qas[:10]],10,**gen_kwargs)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmplus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
