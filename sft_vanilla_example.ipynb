{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常规SFT的微调样例代码\n",
    "\n",
    "## 环境配置\n",
    "\n",
    "系统为`Ubuntu 20.04`\n",
    "CUDA为12.4，驱动是550.54.15\n",
    "\n",
    "### 安装Conda\n",
    "```bash\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "bash Miniconda3-latest-Linux-x86_64.sh\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "### 创建虚拟环境、添加一些镜像列表\n",
    "```bash\n",
    "# conda环境的镜像\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/\n",
    "conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/\n",
    "conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/free/\n",
    "conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/main/\n",
    "conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud/conda-forge/\n",
    "# pip镜像\n",
    "pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/\n",
    "pip config set global.index-url http://mirrors.cloud.tencent.com/pypi/simple\n",
    "pip config set global.index-url http://pypi.douban.com/simple/\n",
    "pip config set global.index-url https://mirrors.163.com/pypi/simple/\n",
    "# 创建虚拟环境\n",
    "conda create -n <环境名称> python=3.12\n",
    "# 激活你创建的环境\n",
    "conda activate <环境名称>\n",
    "```\n",
    "\n",
    "### 安装运行库\n",
    "```bash\n",
    "# pytorch安装\n",
    "pip install --upgrade torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124 --trusted-host download.pytorch.org\n",
    "# 其他的依赖安装\n",
    "pip install --upgrade torch==2.6.0 transformers BitsandBytes accelerate nltk numpy pandas tensorboardX evaluate scikit-learn sentence-transformers tiktoken deepspeed SentencePiece unsloth qwen-vl-utils[decord] nvitop trl -i https://mirrors.aliyun.com/pypi/simple/\n",
    "# 如果用jupyter notebook 需要安装以下两个\n",
    "conda install -n testllm ipykernel --update-deps --force-reinstall\n",
    "conda install -n testllm IProgress\n",
    "```\n",
    "\n",
    "## 配置GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, argparse,json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\" # 这里替换成你想要的GPU序号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入包体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    LogitsProcessorList,\n",
    "    InfNanRemoveLogitsProcessor,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.trainer import TRAINER_STATE_NAME\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers.trainer_utils import SaveStrategy\n",
    "from transformers.tokenization_utils import PaddingStrategy\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n",
    "from peft.utils import (\n",
    "    CONFIG_NAME,\n",
    "    SAFETENSORS_WEIGHTS_NAME,\n",
    "    TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, IterableDataset\n",
    "from dataclasses import dataclass\n",
    "# from 模板 import get_template_and_model_path\n",
    "# from public_apis.file_rw import *\n",
    "from typing import Union, Optional, List, Dict, Any, Literal, Sequence, Tuple\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "IGNORE_INDEX=-100# 引入这个常数，有时候在梯度下降时用于屏蔽无需计算梯度的token\n",
    "\n",
    "TEMPLATE_DICT={\n",
    "    'llama2':\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \\n{user_query}\\nAssistant:  \",\n",
    "    'llama3':\"<|start_header_id|>system<|end_header_id|>\\nYou are a helpful assistant. <|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\\n{user_query}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\",\n",
    "    'qwen2.5':\"<|im_start|>system\\nYou are a helpful assistant. <|im_end|>\\n<|im_start|>user\\n{user_query}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    'gpt2':\"{user_query}\\n\"\n",
    "}\n",
    "\n",
    "MODEL_BASE_PATH='/data'\n",
    "\n",
    "# MODEL_BASE_PATH='/data/lzy/models/LLM'\n",
    "\n",
    "MODEL_NAME_DICT={\n",
    "    'llama2-7b':os.path.join(MODEL_BASE_PATH,'llama','llama-2-7b-hf'),\n",
    "    'llama2-13b':os.path.join(MODEL_BASE_PATH,'llama','llama-2-13b-hf'),\n",
    "    'llama3.1-8b-i':os.path.join(MODEL_BASE_PATH,'llama','llama3.1-8b-instruct'),\n",
    "    'llama3.1-8b':os.path.join(MODEL_BASE_PATH,'llama','llama3.1-8b'),\n",
    "    'llama3.2-1b-i':os.path.join(MODEL_BASE_PATH,'llama','llama3.2-1B-Instruct'),\n",
    "    'llama3.2-3b-i':os.path.join(MODEL_BASE_PATH,'llama','llama3.2-3B-Instruct'),\n",
    "    'qwen2.5-0.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-0.5B-Instruct'),\n",
    "    'qwen2.5-1.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-1.5B-Instruct'),\n",
    "    'qwen2.5-3b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-3B-Instruct'),\n",
    "    'qwen2.5-7b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-7B-Instruct'),\n",
    "    'qwen2.5-14b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-14B-Instruct'),\n",
    "    'qwen2.5-coder-0.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-0.5B-Instruct'),\n",
    "    'qwen2.5-coder-1.5b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-1.5B-Instruct'),\n",
    "    'qwen2.5-coder-3b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-3B-Instruct'),\n",
    "    'qwen2.5-coder-7b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-7B-Instruct'),\n",
    "    'qwen2.5-coder-14b-i':os.path.join(MODEL_BASE_PATH,'Qwen','Qwen2.5-Coder-14B-Instruct'),\n",
    "    # 'gpt2l':os.path.join(MODEL_BASE_PATH,'openai-community','gpt2-large'),\n",
    "    # 'gpt2':os.path.join(MODEL_BASE_PATH,'openai-community','gpt2')\n",
    "}\n",
    "# 得到模板\n",
    "def get_template_and_model_path(**arguments):\n",
    "    model_name=str(arguments.get('model_name','llama3.1-8b-i'))\n",
    "    model_name_or_path=MODEL_NAME_DICT[model_name]\n",
    "    if model_name.startswith('qwen2.5'):\n",
    "        template=TEMPLATE_DICT['qwen2.5']\n",
    "    elif model_name.startswith('llama2'):\n",
    "        template=TEMPLATE_DICT['llama2']\n",
    "    elif model_name.startswith('gpt2'):\n",
    "        template=TEMPLATE_DICT['gpt2']\n",
    "    else:\n",
    "        template=TEMPLATE_DICT['llama3']\n",
    "    return model_name,model_name_or_path,template\n",
    "# 自定义一种错误形式，主要用于防呆以及简化操作\n",
    "class CustomDatasetTokenizerError(Exception):\n",
    "    \"\"\"Base class for custom exceptions in this module.\"\"\"\n",
    "    def __init__(self):\n",
    "        # self.expression = expression\n",
    "        self.message = \"数据集元素的输入键值必须是以下组合：\\n\\t\\\"prompt\\\",\\\"query\\\",\\\"response\\\"\\n\\t\\\"instruction\\\",\\\"input\\\",\\\"output\\\",\\n\\t\\\"prompt\\\",\\\"completion\\\"\"\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "    pass\n",
    "# prompt_template应该是中间有个关键词叫做 `user_query` \n",
    "# 这个函数用于常规的微调（手搓，然后用`transformers`中的`trainer`或者`seq2seqtrainer`直接微调）\n",
    "# 从载入的JSON文件一步变为令牌化的数据集样式\n",
    "def process_tokens_tokenizer_functions(example: dict[str, str],tokenizer: Union[PreTrainedTokenizer,PreTrainedTokenizerBase,PreTrainedTokenizerFast,AutoTokenizer],prompt_template: str,train_mode: Literal[\"sft\", \"pt\"] = \"sft\"):\n",
    "    prompt=\"\"\n",
    "    completion=\"\"\n",
    "    if \"prompt\" in example.keys() and \"query\" in example.keys() and \"response\" in example.keys():\n",
    "        prompt=prompt_template.format(user_query=\"{}\\n{}\\n\".format(example[\"prompt\"], example[\"query\"]))\n",
    "        completion=example[\"response\"]\n",
    "    elif \"instruction\" in example.keys() and \"input\" in example.keys() and \"output\" in example.keys():\n",
    "        prompt=prompt_template.format(user_query=\"{}\\n{}\\n\".format(example[\"instruction\"], example[\"input\"]))\n",
    "        completion=example[\"output\"]\n",
    "    elif \"prompt\" in example.keys() and \"completion\" in example.keys():\n",
    "        prompt=prompt_template.format(user_query=\"{}\\n\".format(example[\"prompt\"]))\n",
    "        completion=example[\"completion\"]\n",
    "    else:\n",
    "        raise CustomDatasetTokenizerError()\n",
    "    # SFT样本构建\n",
    "    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    # 找出非标签部分\n",
    "    example_str = \"{}{}\".format(prompt, completion)\n",
    "    input_id = tokenizer.encode(example_str)\n",
    "    # 训练数据集末尾带上截止符号\n",
    "    if input_id[-1] != tokenizer.eos_token_id:\n",
    "        input_id.append(tokenizer.eos_token_id)\n",
    "    input_id_before_label = tokenizer.encode(prompt)\n",
    "    label = input_id.copy()\n",
    "    # SFT只需要label部分计算梯度，pt需要所有部分计算梯度\n",
    "    if train_mode == \"sft\":\n",
    "        label[: len(input_id_before_label)] = [IGNORE_INDEX] * len(\n",
    "            input_id_before_label\n",
    "        )\n",
    "    attention_mask = [1] * len(input_id)\n",
    "    # 放入输入字典中\n",
    "    if \"qid\" in example.keys():\n",
    "        model_inputs[\"qid\"] = example[\"qid\"]\n",
    "    elif \"ID\" in example.keys():\n",
    "        model_inputs[\"qid\"] = example[\"ID\"]\n",
    "    model_inputs[\"input_ids\"] = input_id\n",
    "    model_inputs[\"attention_mask\"] = attention_mask\n",
    "    model_inputs[\"labels\"] = label\n",
    "    return model_inputs\n",
    "# 直接处理成`trl`可接受的格式（直接文本就行，剩下的交给剩下）\n",
    "def process_dataset_functions(example: dict[str, str]):\n",
    "    prompt=\"\"\n",
    "    completion=\"\"\n",
    "    if \"prompt\" in example.keys() and \"query\" in example.keys() and \"response\" in example.keys():\n",
    "        prompt=\"{}\\n{}\\n\".format(example[\"prompt\"], example[\"query\"])\n",
    "        completion=example[\"response\"]\n",
    "    elif \"instruction\" in example.keys() and \"input\" in example.keys() and \"output\" in example.keys():\n",
    "        prompt=\"{}\\n{}\\n\".format(example[\"instruction\"], example[\"input\"])\n",
    "        completion=example[\"output\"]\n",
    "    elif \"prompt\" in example.keys() and \"completion\" in example.keys():\n",
    "        prompt=\"{}\\n\".format(example[\"prompt\"])\n",
    "        completion=example[\"completion\"]\n",
    "    else:\n",
    "        raise CustomDatasetTokenizerError()\n",
    "    return {\"prompt\":prompt,\"completion\":completion,\"text\":\"### Instruction:\\n {}\\n ### Response:\\n {}\\n\".format(prompt,completion),\"ground_truth\":completion}\n",
    "\n",
    "# 查找检查点需要的正则表达式\n",
    "CHECKPOINT_FOLD = re.compile(r\"(?:checkpoint\\-\\d+)\")\n",
    "FINALMODEL_NAME = re.compile(r\"(?:(?:(?:adapter|pytorch)_)?model(?:\\-\\d+)?\\.(?:safetensors|bin))\")\n",
    "# 查找检查点的函数\n",
    "def checkout_format(string: str, pattern: re.Pattern):\n",
    "    result = [n for n in pattern.findall(string) if n]\n",
    "    if len(result) == 1:\n",
    "        return result[0] == string\n",
    "    return False\n",
    "# 查找检查点的函数\n",
    "def checkpoint_sort_func(checkpoint_dirname: str):\n",
    "    items = [n.strip() for n in checkpoint_dirname.split(\"-\") if n.strip()]\n",
    "    return int(items[-1])\n",
    "# 查找检查点的函数\n",
    "def findout_checkpoint(path: str):\n",
    "    items = os.listdir(path)\n",
    "    saved_files = [n for n in items if checkout_format(n, FINALMODEL_NAME)]\n",
    "    if len(saved_files) > 0:\n",
    "        return path\n",
    "    check_dirs = [n for n in items if checkout_format(n, CHECKPOINT_FOLD)]\n",
    "    if len(check_dirs) < 1:\n",
    "        return None\n",
    "    check_dirs.sort(key=lambda x: checkpoint_sort_func(x), reverse=True)\n",
    "    for f in check_dirs:\n",
    "        result = findout_checkpoint(os.path.join(path, f))\n",
    "        if result != None:\n",
    "            return result\n",
    "    return None\n",
    "# 检查并创建路径\n",
    "def _check_create_dirs(file_path: str):\n",
    "    \"\"\"\n",
    "    # 代码解释\n",
    "    这段代码的功能是检查给定文件路径的父目录是否存在，如果不存在则创建该目录。具体逻辑如下：\n",
    "    1. 使用 `os.path.dirname` 获取文件路径的父目录路径。\n",
    "    2. 如果父目录路径不为空，则进一步检查该路径是否存在。\n",
    "    3. 如果路径不存在，则调用 `os.makedirs` 创建该目录。\n",
    "\n",
    "    # 控制流图\n",
    "    ```mermaid\n",
    "    flowchart TD\n",
    "        A[开始] --> B[获取文件路径的父目录]\n",
    "        B --> C{父目录是否为空}\n",
    "        C -->|是| D[结束]\n",
    "        C -->|否| E{父目录是否存在}\n",
    "        E -->|否| F[创建父目录]\n",
    "        F --> G[结束]\n",
    "        E -->|是| G[结束]\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # 获取文件路径的父目录\n",
    "    p = os.path.dirname(file_path)\n",
    "    # 检查父目录是否为空\n",
    "    if p != '':\n",
    "        # 检查父目录是否存在\n",
    "        if not os.path.exists(p):\n",
    "            # 如果父目录不存在，则创建该目录\n",
    "            os.makedirs(p)\n",
    "    pass\n",
    "def load_from_json(file_path: str) -> Union[None, Dict, List]:\n",
    "    \"\"\"\n",
    "    从指定的文件路径加载JSON数据并解析。\n",
    "\n",
    "    此函数尝试从给定的文件路径中读取JSON数据，如果文件不存在，则返回None。\n",
    "    如果文件存在，它将打开文件，读取内容，并将内容解析为JSON格式的数据，然后返回。\n",
    "    支持返回的数据类型可以是字典、列表或None。\n",
    "\n",
    "    参数:\n",
    "    file_path (str): JSON文件的路径。\n",
    "\n",
    "    返回:\n",
    "    Union[None, Dict, List]: 解析后的JSON数据，如果文件不存在则返回None。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = json.loads(f.read())\n",
    "            f.close()\n",
    "        return content\n",
    "    pass\n",
    "def save_to_json(data: Union[Dict, List], file_path: str):\n",
    "    \"\"\"\n",
    "    将数据保存到JSON文件中。\n",
    "    \n",
    "    该函数接受一个字典或列表形式的数据，并将其序列化为JSON格式，\n",
    "    然后将序列化的数据写入到指定的文件路径中。如果文件路径中的目录不存在，\n",
    "    则会先创建目录。\n",
    "    \n",
    "    参数:\n",
    "    data (Union[Dict, List]): 要保存的字典或列表数据。\n",
    "    file_path (str): 数据保存的文件路径。\n",
    "    \n",
    "    返回:\n",
    "    无返回值。\n",
    "    \"\"\"\n",
    "    _check_create_dirs(file_path)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False, indent=4))\n",
    "        f.close()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数、模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, model_name_or_path, template = get_template_and_model_path(\n",
    "    model_name=\"llama3.1-8b-i\"\n",
    ")\n",
    "\n",
    "output_dir = \"results/normal\"\n",
    "\n",
    "config_dicts = dict(\n",
    "    ignore_pad_token_for_loss=True,\n",
    "    lora_config_dicts=dict(\n",
    "        lora_rank=8, lora_alpha=32, lora_dropout=0.1, additional_target=None\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "training_config_dict = dict(\n",
    "    data_path=\"<你的训练文件地址>\",\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=False,\n",
    "    do_train=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=20,\n",
    "    save_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "    logging_steps=1000,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    resume_from_checkpoint=False,\n",
    "    warmup_ratio=0,\n",
    "    gradient_checkpointing=False,\n",
    ")\n",
    "# 注意，这里没写评估\n",
    "eval_config_dict = dict(\n",
    "    data_path=\"<你的评估文件地址>\",\n",
    "    max_new_tokens=2048,\n",
    ")\n",
    "\n",
    "\n",
    "generating_args = {\n",
    "    \"val_size\": eval_config_dict.get(\"val_size\", 0),\n",
    "    \"streaming\": eval_config_dict.get(\"streaming\", False),\n",
    "    \"buffer_size\": eval_config_dict.get(\"buffer_size\", 16384),\n",
    "    \"do_sample\": eval_config_dict.get(\"do_sample\", True),\n",
    "    \"temperature\": eval_config_dict.get(\"temperature\", 1),\n",
    "    \"top_p\": eval_config_dict.get(\"top_p\", 0.7),\n",
    "    \"top_k\": eval_config_dict.get(\"top_k\", 50),\n",
    "    \"num_beams\": eval_config_dict.get(\n",
    "        \"num_beams\", 8\n",
    "    ),  # 训练的时候为`None`，推理的时候大于0\n",
    "    \"max_new_tokens\": eval_config_dict.get(\"max_new_tokens\", 1024),\n",
    "    \"repetition_penalty\": eval_config_dict.get(\"repetition_penalty\", 1.0),\n",
    "    \"length_penalty\": eval_config_dict.get(\"length_penalty\", 1.0),\n",
    "    # \"num_beam_groups\":4,\n",
    "    # \"diversity_penalty\":0.2\n",
    "}\n",
    "\n",
    "lora_config_dicts = config_dicts[\"lora_config_dicts\"]\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_config_dicts[\"lora_rank\"],\n",
    "    lora_alpha=lora_config_dicts[\"lora_alpha\"],\n",
    "    lora_dropout=lora_config_dicts[\"lora_dropout\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    modules_to_save=lora_config_dicts.get(\"additional_target\", None),\n",
    ")\n",
    "prompt_template = template\n",
    "\n",
    "model_path = model_name_or_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"cuda\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = get_peft_model(model, lora_config, \"default\")\n",
    "loadpath = findout_checkpoint(output_dir)\n",
    "if loadpath != None:\n",
    "    print(\"从`{}`加载lora头\".format(loadpath))\n",
    "    model.load_adapter(loadpath, \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "输入的数据集得的结构是以下三种类型的：\n",
    "\n",
    "1. 键值同时包含（第一优先级）： `prompt`,`query`,`response`\n",
    "2. 键值同时包含（第二优先级）： `instruction`,`input`,`output`\n",
    "3. 键值同时包含（第三优先级）： `prompt`,`completion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=Dataset.from_list([process_tokens_tokenizer_functions(n,tokenizer,prompt_template,training_config_dict.get('stage','sft')) for n in tqdm(load_from_json(training_config_dict[\"data_path\"])) ])\n",
    "print(tokenizer.decode(train_dataset[0]['input_ids']) )\n",
    "tokenizer.padding_side=\"right\"\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model,label_pad_token_id=(IGNORE_INDEX if training_config_dict.get(\"ignore_pad_token_for_loss\",True) else tokenizer.pad_token_id))\n",
    "generation_config=GenerationConfig(**generating_args)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=training_config_dict[\"output_dir\"],\n",
    "    overwrite_output_dir=training_config_dict.get(\"overwrite_output_dir\",False),\n",
    "    do_train=True,\n",
    "    lr_scheduler_type=training_config_dict.get(\"lr_scheduler_type\",\"cosine\"),\n",
    "    learning_rate=training_config_dict.get(\"learning_rate\",1e-4),\n",
    "    num_train_epochs=training_config_dict.get(\"num_train_epochs\",20),\n",
    "    save_steps=training_config_dict.get(\"save_steps\",1000),\n",
    "    per_device_train_batch_size=training_config_dict.get(\"batch_size\",4),\n",
    "    gradient_accumulation_steps=training_config_dict.get(\"gradient_accumulation_steps\",4),\n",
    "    per_device_eval_batch_size=training_config_dict.get(\"batch_size\",4),\n",
    "    fp16= isinstance(training_config_dict.get(\"compute_dtype\",torch.float16),torch.dtype),\n",
    "    bf16= not isinstance(training_config_dict.get(\"compute_dtype\",torch.float16),torch.dtype),\n",
    "    logging_dir=training_config_dict.get(\"logging_dir\",None),\n",
    "    logging_steps=training_config_dict.get(\"logging_steps\",1000),\n",
    "    ddp_find_unused_parameters=False,\n",
    "    resume_from_checkpoint=True,\n",
    "    warmup_ratio=training_config_dict.get(\"warmup_ratio\",0),\n",
    "    gradient_checkpointing=training_config_dict.get(\"gradient_checkpointing\",False),\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=None,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "trainer.train()\n",
    "trainer.save_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
